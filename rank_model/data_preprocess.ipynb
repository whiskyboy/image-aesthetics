{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算并存储每张图片的score\n",
    "import numpy as np\n",
    "\n",
    "def get_score(zan_num, cai_num, clk_num):\n",
    "    zan_num = max(zan_num - 1, 0)\n",
    "    return zan_num - cai_num + np.round(np.log(clk_num+1))\n",
    "\n",
    "img_score_by_date = {}\n",
    "with open(\"../data/img_attr.csv\", 'r') as fin:\n",
    "    for line in fin:\n",
    "        fields = line.strip().split(\"\\t\")\n",
    "        if len(fields) != 7:\n",
    "            continue\n",
    "        imgid = fields[0]\n",
    "        zan_num = int(fields[1])\n",
    "        cai_num = int(fields[2])\n",
    "        clk_num = int(fields[3])\n",
    "        score = get_score(zan_num, cai_num, clk_num)\n",
    "        date = fields[5]\n",
    "        if date >= \"2015/01\" and date < \"2017/01\":\n",
    "            img_score_by_date.setdefault(date, [])\n",
    "            img_score_by_date[date].append((imgid, score))\n",
    "\n",
    "print \"#count of images: %d\"%sum(map(len, img_score_by_date.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构造用于模型训练和验证的pair数据对\n",
    "import random\n",
    "\n",
    "def split_dataset(dataset, p):\n",
    "    \"\"\"\n",
    "    return train_dataset, valid_dataset\n",
    "    \"\"\"\n",
    "    random.shuffle(dataset)\n",
    "    valid_size = int(len(dataset)*p)\n",
    "    return dataset[:-valid_size], dataset[-valid_size:]\n",
    "\n",
    "train_list = []\n",
    "valid1_list = []\n",
    "valid2_list = []\n",
    "p = 0.1 # valid_data ratio\n",
    "k = 15 # compare with k images for each train image\n",
    "delta = 0.5 # if abs(sa-sb) < delta, then abort this comparation pair\n",
    "for date, imgs in img_score_by_date.items():\n",
    "    train_imgs, valid_imgs = split_dataset(imgs, p)\n",
    "    cmp_k = len(train_imgs) > k and k or len(train_imgs)\n",
    "    \n",
    "    for img_a, s_a in train_imgs:\n",
    "        cmp_imgs = random.sample(train_imgs, cmp_k)\n",
    "        for img_b, s_b in cmp_imgs:\n",
    "            if abs(s_a-s_b) < delta:\n",
    "                continue\n",
    "            cmp_ret = s_a > s_b and 1 or 0\n",
    "            train_list.append((img_a, s_a, img_b, s_b, cmp_ret))\n",
    "            \n",
    "    for img_a, s_a in valid_imgs:\n",
    "        cmp_imgs = random.sample(train_imgs, cmp_k)\n",
    "        for img_b, s_b in cmp_imgs:\n",
    "            if abs(s_a-s_b) < delta:\n",
    "                continue\n",
    "            cmp_ret = s_a > s_b and 1 or 0\n",
    "            valid1_list.append((img_a, s_a, img_b, s_b, cmp_ret))\n",
    "            \n",
    "    for img_a, s_a in valid_imgs:\n",
    "        img_b, s_b = random.choice(valid_imgs)\n",
    "        if abs(s_a-s_b) < delta:\n",
    "            continue\n",
    "        cmp_ret = s_a > s_b and 1 or 0\n",
    "        valid2_list.append((img_a, s_a, img_b, s_b, cmp_ret))\n",
    "            \n",
    "print \"Length of Train List: %d\"%len(train_list)\n",
    "print \"Length of Valid1 List: %d\"%len(valid1_list)\n",
    "print \"Length of Valid2 List: %d\"%len(valid2_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构造用于模型训练和验证的pair数据对--V2\n",
    "# 将训练数据集分成两个part\n",
    "# 对于partB中的每个图片，依次从partA中取01等量的图片用于构造训练数据集\n",
    "# 这样可以避免partB中数据01分类不均带来的误拟合问题\n",
    "# 验证集则是将每张图片同partB里的图片进行比较，partA里的图片不出现在验证集合里\n",
    "import random\n",
    "import cPickle as pickle\n",
    "\n",
    "max_pair_num = 10 # 对于每张图片最多构造10×2张训练样本\n",
    "min_imgs_num = 50 # 如果当天的照片数量小于50张，则认为该天的数据量不足，弃之不用\n",
    "valid_p = 0.1 # 验证集占的比例\n",
    "\n",
    "def split_dataset(dataset):\n",
    "    \"\"\"\n",
    "    return train_dataset, valid_dataset\n",
    "    \"\"\"\n",
    "    if len(dataset) < min_imgs_num:\n",
    "        return ([], [], [])\n",
    "    random.shuffle(dataset)\n",
    "    valid_size = int(len(dataset)*valid_p)\n",
    "    trainset, validset = dataset[:-valid_size], dataset[-valid_size:]\n",
    "    trainsetA, trainsetB = trainset[:len(trainset)/2], trainset[len(trainset)/2:]\n",
    "    return trainsetA, trainsetB, validset\n",
    "\n",
    "def img_cmp(trainsetA, img, score):\n",
    "    img_pairs = []\n",
    "    high_imgs = []\n",
    "    low_imgs = []\n",
    "    for imgA, scoreA in trainsetA:\n",
    "        if scoreA > score:\n",
    "            high_imgs.append((imgA, scoreA))\n",
    "        elif scoreA < score:\n",
    "            low_imgs.append((imgA, scoreA))\n",
    "    pair_num = min(max_pair_num, len(low_imgs), len(high_imgs))\n",
    "    for imgA, scoreA in random.sample(high_imgs, pair_num):\n",
    "        #if random.random() < 0.5:\n",
    "        img_pairs.append((imgA, scoreA, img, score, 1))\n",
    "        #else:\n",
    "        img_pairs.append((img, score, imgA, scoreA, 0))\n",
    "    for imgA, scoreA in random.sample(low_imgs, pair_num):\n",
    "        #if random.random() < 0.5:\n",
    "        img_pairs.append((imgA, scoreA, img, score, 0))\n",
    "        #else:\n",
    "        img_pairs.append((img, score, imgA, scoreA, 1))\n",
    "    return img_pairs\n",
    "\n",
    "train_list = []\n",
    "valid_list = []\n",
    "\n",
    "for date, imgs in img_score_by_date.items():\n",
    "    trainsetA, trainsetB, validset = split_dataset(imgs)\n",
    "    valid_trainsetB = []\n",
    "    for imgB, scoreB in trainsetB:\n",
    "        img_pairs = img_cmp(trainsetA, imgB, scoreB)\n",
    "        if len(img_pairs) != 0:\n",
    "            valid_trainsetB.append((imgB, scoreB))\n",
    "            train_list.extend(img_pairs)\n",
    "            \n",
    "    for imgV, scoreV in validset:\n",
    "        pair_num = min(len(valid_trainsetB), max_pair_num)\n",
    "        for imgB, scoreB in random.sample(valid_trainsetB, pair_num):\n",
    "            if scoreV > scoreB:\n",
    "                valid_list.append((imgV, scoreV, imgB, scoreB, 1))\n",
    "            elif scoreV < scoreB:\n",
    "                valid_list.append((imgV, scoreV, imgB, scoreB, 0))\n",
    "            \n",
    "print \"Length of Train List: %d\"%len(train_list)\n",
    "print \"Length of Valid List: %d\"%len(valid_list)\n",
    "\n",
    "pickle.dump(train_list, open(\"./data/train.list\", 'wb'))\n",
    "pickle.dump(valid_list, open(\"./data/valid.list\", 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试：训练数据集的合理性\n",
    "%matplotlib inline\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import os\n",
    "\n",
    "img_path = \"../data/img/\"\n",
    "def cmpPlot(imgid_a, s_a, imgid_b, s_b):\n",
    "    plt.figure()\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.title(\"%s_%s\"%(imgid_a, s_a))\n",
    "    img = Image.open(img_path+\"%s.jpg\"%imgid_a)\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.title(\"%s_%s\"%(imgid_b, s_b))\n",
    "    img = Image.open(img_path+\"%s.jpg\"%imgid_b)\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "for imgA, sA, imgB, sB, cmpret in random.sample(train_list, 30):\n",
    "    if os.path.exists(img_path+\"%s.jpg\"%imgA) and os.path.exists(img_path+\"%s.jpg\"%imgB):\n",
    "        cmpPlot(imgA, sA, imgB, sB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 定义InvecptionV3的预处理模型\n",
    "from keras.models import Model\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.layers import GlobalAveragePooling2D\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.inception_v3 import preprocess_input\n",
    "import numpy as np\n",
    "\n",
    "base_model = InceptionV3(weights='imagenet', include_top=False)\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x) # add a global spatial average pooling layer\n",
    "model = Model(inputs=base_model.input, outputs=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 对每张照片预处理并存储在lmdb里\n",
    "import lmdb\n",
    "\n",
    "batch_size = 128\n",
    "img_path = \"../data/img/\"\n",
    "\n",
    "imgid_set = set(map(lambda x: x[0], train_list) + \\\n",
    "            map(lambda x: x[2], train_list) + \\\n",
    "            map(lambda x: x[0], valid_list) + \\\n",
    "            map(lambda x: x[2], valid_list))\n",
    "env = lmdb.open(\"./data/features\", map_size=8192*3*len(imgid_set))\n",
    "\n",
    "def preprocess(imgid):\n",
    "    try:\n",
    "        filename = img_path+\"%s.jpg\"%imgid\n",
    "        img = image.load_img(filename, target_size=(299, 299))\n",
    "        x = image.img_to_array(img)\n",
    "        x = preprocess_input(x)\n",
    "        return x\n",
    "    except Exception, e:\n",
    "        print str(e)\n",
    "        return None\n",
    "\n",
    "imgids = []\n",
    "X = []\n",
    "for i, imgid in enumerate(imgid_set):\n",
    "    x = preprocess(imgid)\n",
    "    if x is None:\n",
    "        continue\n",
    "    imgids.append(imgid)\n",
    "    X.append(x)\n",
    "    if len(X) == batch_size:\n",
    "        features = model.predict_on_batch(np.array(X))\n",
    "        \n",
    "        txn = env.begin(write=True)\n",
    "        for _imgid, _feature in zip(imgids, features):\n",
    "            str_feature = _feature.tostring()\n",
    "            txn.put(_imgid, str_feature)\n",
    "        txn.commit()\n",
    "        \n",
    "        imgids = []\n",
    "        X = []\n",
    "        print \"%d/%d\"%(i, len(imgid_set))\n",
    "        \n",
    "if len(X) > 0:\n",
    "    features = model.predict_on_batch(np.array(X))\n",
    "    txn = env.begin(write=True)\n",
    "    for _imgid, _feature in zip(imgids, features):\n",
    "        str_feature = _feature.tostring()\n",
    "        txn.put(_imgid, str_feature)\n",
    "    txn.commit()\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
