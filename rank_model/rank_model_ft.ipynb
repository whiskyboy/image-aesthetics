{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "WARNING (theano.sandbox.cuda): The cuda backend is deprecated and will be removed in the next release (v0.10).  Please switch to the gpuarray backend. You can get more information about how to switch at this URL:\n",
      " https://github.com/Theano/Theano/wiki/Converting-to-the-new-gpu-back-end%28gpuarray%29\n",
      "\n",
      "Using gpu device 0: GeForce GTX 1060 6GB (CNMeM is disabled, cuDNN 5110)\n"
     ]
    }
   ],
   "source": [
    "# define the model\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.inception_v3 import preprocess_input\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Input, Dense, concatenate, GlobalAveragePooling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.core import Dropout\n",
    "from keras import backend as K\n",
    "from keras import optimizers\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img_feature_a = Input(shape=(2048,))\n",
    "img_feature_b = Input(shape=(2048,))\n",
    "\n",
    "shared_fc_layer = Sequential([\n",
    "    Dense(1024, activation='relu', input_shape=(2048, )),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.5),\n",
    "    Dense(256, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.5),\n",
    "])\n",
    "\n",
    "encoded_a = shared_fc_layer(img_feature_a)\n",
    "encoded_b = shared_fc_layer(img_feature_b)\n",
    "\n",
    "merged_vector = concatenate([encoded_a, encoded_b])\n",
    "\n",
    "x = Dense(256, activation='relu')(merged_vector)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "output = Dense(2, activation='softmax')(x)\n",
    "\n",
    "rank_model = Model(inputs=[img_feature_a, img_feature_b], outputs=output)\n",
    "\n",
    "rank_model.load_weights(\"./data/rank_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_4 (InputLayer)             (None, 3, 299, 299)   0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_5 (InputLayer)             (None, 3, 299, 299)   0                                            \n",
      "____________________________________________________________________________________________________\n",
      "model_2 (Model)                  (None, 2048)          21802784    input_4[0][0]                    \n",
      "                                                                   input_5[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "model_1 (Model)                  (None, 2)             2498562     model_2[1][0]                    \n",
      "                                                                   model_2[2][0]                    \n",
      "====================================================================================================\n",
      "Total params: 24,301,346\n",
      "Trainable params: 13,610,370\n",
      "Non-trainable params: 10,690,976\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "base_model = InceptionV3(weights='imagenet', include_top=False)\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x) # add a global spatial average pooling layer\n",
    "inception_layer = Model(inputs=base_model.input, outputs=x)\n",
    "\n",
    "img_a = Input(shape=(3, 299, 299))\n",
    "img_b = Input(shape=(3, 299, 299))\n",
    "\n",
    "img_feature_a = inception_layer(img_a)\n",
    "img_feature_b = inception_layer(img_b)\n",
    "\n",
    "output = rank_model([img_feature_a, img_feature_b])\n",
    "\n",
    "model = Model(inputs=[img_a, img_b], outputs=output)\n",
    "\n",
    "for layer in base_model.layers[:249]:\n",
    "    layer.trainable = False\n",
    "for layer in base_model.layers[249:]:\n",
    "    layer.trainable = True\n",
    "    \n",
    "\n",
    "optimizer = optimizers.SGD(lr=0.0001, momentum=0.9)\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img_path = \"../data/img/\"\n",
    "def img_preprocess(imgid, target_size=(299, 299)):\n",
    "    try:\n",
    "        filename = img_path+\"%s.jpg\"%imgid\n",
    "        img = image.load_img(filename, target_size=target_size)\n",
    "        x = image.img_to_array(img)\n",
    "        x = preprocess_input(x)\n",
    "        return x\n",
    "    except Exception, e:\n",
    "        print str(e)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 150716 images belonging to 1 classes.\n",
      "Found 9216 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import *\n",
    "import os\n",
    "\n",
    "# 自定义DirectoryIterator类，可以返回自定义的label\n",
    "class CustomDirectoryIterator(DirectoryIterator):  \n",
    "    def next(self):\n",
    "        \"\"\"For python 2.x.\n",
    "        # Returns\n",
    "            The next batch.\n",
    "        \"\"\"\n",
    "        with self.lock:\n",
    "            index_array, current_index, current_batch_size = next(self.index_generator)\n",
    "        # The transformation of images is not under thread lock\n",
    "        # so it can be done in parallel\n",
    "        batch_x1 = np.zeros((current_batch_size,) + self.image_shape, dtype=K.floatx())\n",
    "        batch_x2 = np.zeros((current_batch_size,) + self.image_shape, dtype=K.floatx())\n",
    "        batch_y = np.zeros((current_batch_size, 2) , dtype=K.floatx())\n",
    "        # build batch of image data\n",
    "        for i, j in enumerate(index_array):\n",
    "            fname =  self.filenames[j]\n",
    "            fname = os.path.basename(fname)\n",
    "            fname, _ = os.path.splitext(fname)\n",
    "            imgA, imgB, cmpret = fname.split(\"_\")\n",
    "            x1 = img_preprocess(imgA, self.target_size)\n",
    "            x2 = img_preprocess(imgB, self.target_size)\n",
    "            batch_x1[i] = x1\n",
    "            batch_x2[i] = x2\n",
    "            batch_y[i, int(cmpret)] = 1\n",
    "        return [batch_x1, batch_x2], batch_y\n",
    "\n",
    "# 定义批处理的数据集大小：较小的batch_size可以增加权重调整的次数，同时节省内存的开销\n",
    "batch_size = 16 \n",
    "\n",
    "# 图片预处理工具类\n",
    "IDG = ImageDataGenerator()\n",
    "\n",
    "# 从目录文件中流式读取数据，避免训练中一次性加载爆内存\n",
    "train_batch = CustomDirectoryIterator(\"./data/finetune/train/\", IDG, \n",
    "                                      target_size=(299, 299), batch_size=batch_size, shuffle=True)\n",
    "valid_batch = CustomDirectoryIterator(\"./data/finetune/valid/\", IDG, \n",
    "                                      target_size=(299, 299), batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 验证模型的初始化与rank model的结果相一致\n",
    "import os\n",
    "import cPickle as pickle\n",
    "\n",
    "Y_predict_by_rank_model = pickle.load(open(\"./data/valid_predict_by_rank_model.pick\", 'rb'))\n",
    "Y_predict = model.predict_generator(valid_batch, steps=valid_batch.samples // batch_size + 1)\n",
    "\n",
    "for i, fname in enumerate(valid_batch.filenames):\n",
    "    fname = os.path.basename(fname)\n",
    "    fname, _ = os.path.splitext(fname)\n",
    "    imgA, imgB, _ = fname.split(\"_\")\n",
    "    if (imgA, imgB) not in Y_predict_by_rank_model:\n",
    "        continue\n",
    "    y_r = Y_predict_by_rank_model[(imgA, imgB)]\n",
    "    y = Y_predict[i, 1]\n",
    "    print y_r, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lmdb\n",
    "\n",
    "env = lmdb.open(\"./data/features\")\n",
    "txn = env.begin()\n",
    "\n",
    "def testOutput(fname):\n",
    "    fname = os.path.basename(fname)\n",
    "    fname, _ = os.path.splitext(fname)\n",
    "    imgA, imgB, cmpret = fname.split(\"_\")\n",
    "    \n",
    "    rrr = model.predict([np.array([x1]), np.array([x2])])\n",
    "    \n",
    "    x1 = img_preprocess(imgA)\n",
    "    x2 = img_preprocess(imgB)\n",
    "    f1 = inception_layer.predict(np.array([x1]))\n",
    "    f2 = inception_layer.predict(np.array([x2]))\n",
    "    r = rank_model.predict([f1, f2])  \n",
    "    \n",
    "    fa = txn.get(imgA)\n",
    "    fb = txn.get(imgB)\n",
    "    fa = np.fromstring(fa, np.float32)\n",
    "    fb = np.fromstring(fb, np.float32)\n",
    "    rr = Y_predict_by_rank_model[(imgA, imgB)]\n",
    "    return f1, f2, r, fa, fb, rr, rrr\n",
    "\n",
    "testOutput(valid_batch.filenames[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "9419/9419 [==============================] - 4733s - loss: 0.6601 - acc: 0.6233 - val_loss: 0.6731 - val_acc: 0.5913\n",
      "Epoch 2/10\n",
      "9419/9419 [==============================] - 4840s - loss: 0.6057 - acc: 0.6723 - val_loss: 0.6848 - val_acc: 0.5916\n",
      "Epoch 3/10\n",
      "9419/9419 [==============================] - 4840s - loss: 0.5584 - acc: 0.7170 - val_loss: 0.7084 - val_acc: 0.5919\n",
      "Epoch 4/10\n",
      "9419/9419 [==============================] - 4843s - loss: 0.5115 - acc: 0.7525 - val_loss: 0.7383 - val_acc: 0.5864\n",
      "Epoch 5/10\n",
      "9419/9419 [==============================] - 4839s - loss: 0.4644 - acc: 0.7844 - val_loss: 0.7591 - val_acc: 0.5882\n",
      "Epoch 6/10\n",
      "9419/9419 [==============================] - 4840s - loss: 0.4205 - acc: 0.8120 - val_loss: 0.7941 - val_acc: 0.5900\n",
      "Epoch 7/10\n",
      "9419/9419 [==============================] - 4839s - loss: 0.3809 - acc: 0.8330 - val_loss: 0.8079 - val_acc: 0.5856\n",
      "Epoch 8/10\n",
      "4273/9419 [============>.................] - ETA: 2528s - loss: 0.3509 - acc: 0.8501"
     ]
    }
   ],
   "source": [
    "model.fit_generator(train_batch, steps_per_epoch=train_batch.samples // batch_size, epochs=10,\n",
    "                       validation_data=valid_batch, validation_steps=valid_batch.samples // batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save_weights(\"./data/rank_model_ft.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN ONLY ONCE\n",
    "import cPickle as pickle\n",
    "import random\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "train_list = pickle.load(open(\"./data/train.list\", 'rb'))\n",
    "valid_list = pickle.load(open(\"./data/valid.list\", 'rb'))\n",
    "\n",
    "if os.path.exists(\"./data/finetune\"):\n",
    "    shutil.rmtree(\"./data/finetune\")\n",
    "os.makedirs(\"./data/finetune/train/none\")\n",
    "os.makedirs(\"./data/finetune/valid/none\")\n",
    "\n",
    "white_set = set()\n",
    "def touch_files(dataset, path, count=-1):\n",
    "    if count != -1:\n",
    "        subdata = random.sample(dataset, count)\n",
    "    else:\n",
    "        subdata = dataset\n",
    "    for imgA, sA, imgB, sB, cmpret in subdata:\n",
    "        if imgA in white_set or img_preprocess(imgA) is not None:\n",
    "            white_set.add(imgA)\n",
    "        if imgB in white_set or img_preprocess(imgB) is not None:\n",
    "            white_set.add(imgB)\n",
    "        if imgA in white_set and imgB in white_set:\n",
    "            open(path+\"/%s_%s_%d.jpg\"%(imgA, imgB, cmpret), 'w').close()\n",
    "        \n",
    "touch_files(train_list, path=\"./data/finetune/train/none/\", count=100000)\n",
    "touch_files(valid_list, path=\"./data/finetune/valid/none/\", count=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "path = \"./data/finetune/valid/none/\"\n",
    "for fname in os.listdir(path):\n",
    "    if random.random() < 0.8:\n",
    "        os.remove(path+fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
