{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "WARNING (theano.sandbox.cuda): The cuda backend is deprecated and will be removed in the next release (v0.10).  Please switch to the gpuarray backend. You can get more information about how to switch at this URL:\n",
      " https://github.com/Theano/Theano/wiki/Converting-to-the-new-gpu-back-end%28gpuarray%29\n",
      "\n",
      "Using gpu device 0: GeForce GTX 1060 6GB (CNMeM is disabled, cuDNN 5110)\n"
     ]
    }
   ],
   "source": [
    "# define the model\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.inception_v3 import preprocess_input\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Input, Dense, concatenate, GlobalAveragePooling2D, Activation, Add, Lambda\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.core import Dropout\n",
    "from keras import backend as K\n",
    "from keras import optimizers\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_4 (InputLayer)             (None, 3, 299, 299)   0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_5 (InputLayer)             (None, 3, 299, 299)   0                                            \n",
      "____________________________________________________________________________________________________\n",
      "model_2 (Model)                  (None, 2048)          21802784    input_4[0][0]                    \n",
      "                                                                   input_5[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "model_1 (Model)                  (None, 1)             525825      model_2[1][0]                    \n",
      "                                                                   model_2[2][0]                    \n",
      "====================================================================================================\n",
      "Total params: 22,328,609\n",
      "Trainable params: 22,293,665\n",
      "Non-trainable params: 34,944\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "img_feature_a = Input(shape=(2048,))\n",
    "img_feature_b = Input(shape=(2048,))\n",
    "\n",
    "shared_fc_layer = Sequential([\n",
    "    Dense(256, activation='relu', input_shape=(2048, )),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.5),\n",
    "    #Dense(256, activation='relu', input_shape=(2048, )),\n",
    "    #BatchNormalization(),\n",
    "    #Dropout(0.5),\n",
    "    Dense(1),\n",
    "])\n",
    "\n",
    "encoded_a = shared_fc_layer(img_feature_a)\n",
    "encoded_b = shared_fc_layer(img_feature_b)\n",
    "\n",
    "negated_encoded_b = Lambda(lambda x: -x, output_shape=(1,))(encoded_b)\n",
    "diff = Add()([encoded_a, negated_encoded_b])\n",
    "\n",
    "output = Activation(\"sigmoid\")(diff)\n",
    "\n",
    "rank_model = Model(inputs=[img_feature_a, img_feature_b], outputs=output)\n",
    "\n",
    "rank_model.load_weights(\"./data/ranknet/ranknet.h5\")\n",
    "\n",
    "##########################################################################################################################\n",
    "\n",
    "base_model = InceptionV3(weights='imagenet', include_top=False)\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x) # add a global spatial average pooling layer\n",
    "inception_layer = Model(inputs=base_model.input, outputs=x)\n",
    "\n",
    "img_a = Input(shape=(3, 299, 299))\n",
    "img_b = Input(shape=(3, 299, 299))\n",
    "\n",
    "img_feature_a = inception_layer(img_a)\n",
    "img_feature_b = inception_layer(img_b)\n",
    "\n",
    "output = rank_model([img_feature_a, img_feature_b])\n",
    "\n",
    "model = Model(inputs=[img_a, img_b], outputs=output)\n",
    "\n",
    "#for layer in base_model.layers[:249]:\n",
    "#    layer.trainable = False\n",
    "#for layer in base_model.layers[249:]:\n",
    "#    layer.trainable = True\n",
    "\n",
    "#optimizer = optimizers.Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "optimizer = optimizers.SGD(lr=0.0001, momentum=0.9)\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img_path = \"../data/img/\"\n",
    "def img_preprocess(imgid, target_size=(299, 299)):\n",
    "    try:\n",
    "        filename = img_path+\"%s.jpg\"%imgid\n",
    "        img = image.load_img(filename, target_size=target_size)\n",
    "        x = image.img_to_array(img)\n",
    "        x = preprocess_input(x)\n",
    "        return x\n",
    "    except Exception, e:\n",
    "        print str(e)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN ONLY ONCE\n",
    "import cPickle as pickle\n",
    "import random\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "train_list = pickle.load(open(\"./data/ranknet/train.list\", 'rb'))\n",
    "valid_list = pickle.load(open(\"./data/ranknet/valid.list\", 'rb'))\n",
    "\n",
    "if os.path.exists(\"./data/ranknet_ft\"):\n",
    "    shutil.rmtree(\"./data/ranknet_ft\")\n",
    "os.makedirs(\"./data/ranknet_ft/train/none\")\n",
    "os.makedirs(\"./data/ranknet_ft/valid/none\")\n",
    "\n",
    "white_set = set()\n",
    "def touch_files(dataset, path, count=-1):\n",
    "    if count != -1:\n",
    "        subdata = random.sample(dataset, count)\n",
    "    else:\n",
    "        subdata = dataset\n",
    "    for imgA, sA, imgB, sB, cmpret in subdata:\n",
    "        if imgA in white_set or img_preprocess(imgA) is not None:\n",
    "            white_set.add(imgA)\n",
    "        if imgB in white_set or img_preprocess(imgB) is not None:\n",
    "            white_set.add(imgB)\n",
    "        if imgA in white_set and imgB in white_set:\n",
    "            open(path+\"/%s_%s_%d.jpg\"%(imgA, imgB, cmpret), 'w').close()\n",
    "        \n",
    "touch_files(train_list, path=\"./data/ranknet_ft/train/none/\", count=100000)\n",
    "touch_files(valid_list, path=\"./data/ranknet_ft/valid/none/\", count=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 99530 images belonging to 1 classes.\n",
      "Found 4334 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import *\n",
    "import os\n",
    "\n",
    "# 自定义DirectoryIterator类，可以返回自定义的label\n",
    "class CustomDirectoryIterator(DirectoryIterator):  \n",
    "    def next(self):\n",
    "        \"\"\"For python 2.x.\n",
    "        # Returns\n",
    "            The next batch.\n",
    "        \"\"\"\n",
    "        with self.lock:\n",
    "            index_array, current_index, current_batch_size = next(self.index_generator)\n",
    "        # The transformation of images is not under thread lock\n",
    "        # so it can be done in parallel\n",
    "        batch_x1 = np.zeros((current_batch_size,) + self.image_shape, dtype=K.floatx())\n",
    "        batch_x2 = np.zeros((current_batch_size,) + self.image_shape, dtype=K.floatx())\n",
    "        batch_y = np.zeros((current_batch_size, ) , dtype=K.floatx())\n",
    "        # build batch of image data\n",
    "        for i, j in enumerate(index_array):\n",
    "            fname =  self.filenames[j]\n",
    "            fname = os.path.basename(fname)\n",
    "            fname, _ = os.path.splitext(fname)\n",
    "            imgA, imgB, cmpret = fname.split(\"_\")\n",
    "            x1 = img_preprocess(imgA, self.target_size)\n",
    "            x2 = img_preprocess(imgB, self.target_size)\n",
    "            batch_x1[i] = x1\n",
    "            batch_x2[i] = x2\n",
    "            batch_y[i] = int(cmpret)\n",
    "        return [batch_x1, batch_x2], batch_y\n",
    "\n",
    "# 定义批处理的数据集大小：较小的batch_size可以增加权重调整的次数，同时节省内存的开销\n",
    "batch_size = 16 \n",
    "\n",
    "# 图片预处理工具类\n",
    "IDG = ImageDataGenerator()\n",
    "\n",
    "# 从目录文件中流式读取数据，避免训练中一次性加载爆内存\n",
    "train_batch = CustomDirectoryIterator(\"./data/ranknet_ft/train/\", IDG, \n",
    "                                      target_size=(299, 299), batch_size=batch_size, shuffle=True)\n",
    "valid_batch = CustomDirectoryIterator(\"./data/ranknet_ft/valid/\", IDG, \n",
    "                                      target_size=(299, 299), batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "6220/6220 [==============================] - 8619s - loss: 0.6473 - acc: 0.6237 - val_loss: 0.6466 - val_acc: 0.6229\n",
      "Epoch 2/5\n",
      "6220/6220 [==============================] - 8627s - loss: 0.5946 - acc: 0.6821 - val_loss: 0.6746 - val_acc: 0.6179\n",
      "Epoch 3/5\n",
      "6220/6220 [==============================] - 8634s - loss: 0.5192 - acc: 0.7440 - val_loss: 0.7319 - val_acc: 0.6070\n",
      "Epoch 4/5\n",
      "6220/6220 [==============================] - 8630s - loss: 0.4270 - acc: 0.8037 - val_loss: 0.8088 - val_acc: 0.6123\n",
      "Epoch 5/5\n",
      "3324/6220 [===============>..............] - ETA: 3970s - loss: 0.3544 - acc: 0.8457"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-34fb9e452adb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m model.fit_generator(train_batch, steps_per_epoch=train_batch.samples // batch_size, epochs=5,\n\u001b[0;32m----> 2\u001b[0;31m                        validation_data=valid_batch, validation_steps=valid_batch.samples // batch_size)\n\u001b[0m",
      "\u001b[0;32m/home/tian/anaconda2/lib/python2.7/site-packages/keras/legacy/interfaces.pyc\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     87\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 88\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_support_signature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetargspec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/tian/anaconda2/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_q_size, workers, pickle_safe, initial_epoch)\u001b[0m\n\u001b[1;32m   1888\u001b[0m                     outs = self.train_on_batch(x, y,\n\u001b[1;32m   1889\u001b[0m                                                \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1890\u001b[0;31m                                                class_weight=class_weight)\n\u001b[0m\u001b[1;32m   1891\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1892\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/tian/anaconda2/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1631\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1632\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1633\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1634\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1635\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/tian/anaconda2/lib/python2.7/site-packages/keras/backend/theano_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   1156\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1157\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1158\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/tian/anaconda2/lib/python2.7/site-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    882\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    883\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 884\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0moutput_subset\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    885\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    886\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/tian/anaconda2/lib/python2.7/site-packages/theano/ifelse.pyc\u001b[0m in \u001b[0;36mthunk\u001b[0;34m()\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 246\u001b[0;31m         \u001b[0;32mdef\u001b[0m \u001b[0mthunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    247\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcompute_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcond\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit_generator(train_batch, steps_per_epoch=train_batch.samples // batch_size, epochs=5,\n",
    "                       validation_data=valid_batch, validation_steps=valid_batch.samples // batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save_weights(\"./data/ranknet_ft/ranknet_ft.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "path = \"./data/finetune/valid/none/\"\n",
    "for fname in os.listdir(path):\n",
    "    if random.random() < 0.8:\n",
    "        os.remove(path+fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 验证模型的初始化与rank model的结果相一致\n",
    "import os\n",
    "import cPickle as pickle\n",
    "\n",
    "Y_predict_by_rank_model = pickle.load(open(\"./data/valid_predict_by_rank_model.pick\", 'rb'))\n",
    "Y_predict = model.predict_generator(valid_batch, steps=valid_batch.samples // batch_size + 1)\n",
    "\n",
    "for i, fname in enumerate(valid_batch.filenames):\n",
    "    fname = os.path.basename(fname)\n",
    "    fname, _ = os.path.splitext(fname)\n",
    "    imgA, imgB, _ = fname.split(\"_\")\n",
    "    if (imgA, imgB) not in Y_predict_by_rank_model:\n",
    "        continue\n",
    "    y_r = Y_predict_by_rank_model[(imgA, imgB)]\n",
    "    y = Y_predict[i, 1]\n",
    "    print y_r, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import lmdb\n",
    "\n",
    "env = lmdb.open(\"./data/features\")\n",
    "txn = env.begin()\n",
    "\n",
    "def testOutput(fname):\n",
    "    fname = os.path.basename(fname)\n",
    "    fname, _ = os.path.splitext(fname)\n",
    "    imgA, imgB, cmpret = fname.split(\"_\")\n",
    "    \n",
    "    rrr = model.predict([np.array([x1]), np.array([x2])])\n",
    "    \n",
    "    x1 = img_preprocess(imgA)\n",
    "    x2 = img_preprocess(imgB)\n",
    "    f1 = inception_layer.predict(np.array([x1]))\n",
    "    f2 = inception_layer.predict(np.array([x2]))\n",
    "    r = rank_model.predict([f1, f2])  \n",
    "    \n",
    "    fa = txn.get(imgA)\n",
    "    fb = txn.get(imgB)\n",
    "    fa = np.fromstring(fa, np.float32)\n",
    "    fb = np.fromstring(fb, np.float32)\n",
    "    rr = Y_predict_by_rank_model[(imgA, imgB)]\n",
    "    return f1, f2, r, fa, fb, rr, rrr\n",
    "\n",
    "testOutput(valid_batch.filenames[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
