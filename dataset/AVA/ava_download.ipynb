{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-requirements\n",
    "1. 安装[SSDB](https://github.com/jhao104/memory-notes/blob/master/SSDB/SSDB安装配置记录.md) Server并启动服务；\n",
    "2. 部署[proxy_pool](https://github.com/jhao104/proxy_pool)；注意将配置文件里的DB更改为SSDB，同时修改SSDB的Host和Port；启动proxy_pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import urllib2\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib\n",
    "import logging\n",
    "import time\n",
    "import requests\n",
    "import os\n",
    "import json\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "\n",
    "logging.basicConfig(level=logging.INFO,\n",
    "                    format='%(asctime)s %(levelname)s %(message)s',\n",
    "                    datefmt='%m-%d %H:%M')\n",
    "\n",
    "def get_proxy():\n",
    "    return requests.get(\"http://127.0.0.1:5000/get/\").content\n",
    "\n",
    "def get_all_proxy():\n",
    "    try:\n",
    "        return json.loads(requests.get(\"http://127.0.0.1:5000/get_all/\").content)\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "def delete_proxy(proxy):\n",
    "    try:\n",
    "        requests.get(\"http://127.0.0.1:5000/delete/?proxy={}\".format(proxy))\n",
    "        logging.info(\"[proxy]%s is invalid and has been removed.\"%proxy)\n",
    "    except:\n",
    "        logging.error(\"[proxy]%s delete failed\"%proxy)\n",
    "    \n",
    "class WebParser(object):\n",
    "    def __init__(self, wait_second=1, max_retry_time=10):\n",
    "        self.url_pattern = \"http://www.dpchallenge.com/image.php?IMAGE_ID=%s\"\n",
    "        \n",
    "        self.html_cache_path = \"./data/html_cache/\"\n",
    "        self.img_cache_path = \"./data/img_cache/\"\n",
    "        if not os.path.exists(self.html_cache_path):\n",
    "            os.makedirs(self.html_cache_path)\n",
    "        if not os.path.exists(self.img_cache_path):\n",
    "            os.makedirs(self.img_cache_path)\n",
    "        \n",
    "        self.imgid = -1\n",
    "        self.soup = None\n",
    "        \n",
    "        self.wait_second = wait_second\n",
    "        self.max_retry_time = max_retry_time\n",
    "        \n",
    "        self.MIN_HTML_SIZE = 1024\n",
    "        self.MAX_FAILED_CNT = 10\n",
    "        self.failed_proxy = {}\n",
    "        \n",
    "    def __get_html_response(self, url, valid_size=-1):\n",
    "        html = None\n",
    "        for _ in range(self.max_retry_time):\n",
    "            user_agent = \"Mozilla/5.0 (X11; U; Linux x86_64; en-US; rv:1.9.2.24) Gecko/20111109 CentOS/3.6.24-3.el6.centos Firefox/3.6.24\"\n",
    "            headers = {'User-Agent': user_agent}\n",
    "            proxy = get_proxy()\n",
    "            self.failed_proxy.setdefault(proxy, 0)\n",
    "            try:\n",
    "                html = requests.get(url=url, \n",
    "                                    headers=headers, \n",
    "                                    proxies={\"http\": \"http://{}\".format(proxy)})\\\n",
    "                               .content\n",
    "                if len(html) < valid_size:\n",
    "                    html = None\n",
    "                    raise Exception(\"Invalid html response!\")\n",
    "            except Exception, e:\n",
    "                self.failed_proxy[proxy] += 1\n",
    "                logging.error(\"[proxy]%s failed %d times\"%(proxy, self.failed_proxy[proxy]))\n",
    "            else:\n",
    "                self.failed_proxy[proxy] = 0\n",
    "                break\n",
    "            finally:\n",
    "                time.sleep(self.wait_second)\n",
    "        return html\n",
    "    \n",
    "\n",
    "    def load_html(self, imgid):\n",
    "        self.imgid = imgid\n",
    "        cache_file = self.html_cache_path + \"%s.html\"%self.imgid\n",
    "        if os.path.exists(cache_file):\n",
    "            logging.info(\"[imgid=%s]html has been cached.\"%self.imgid)\n",
    "            html = open(cache_file, 'r').read()\n",
    "        else:\n",
    "            url = self.url_pattern%self.imgid\n",
    "            html = self.__get_html_response(url, valid_size=self.MIN_HTML_SIZE)\n",
    "            if html is None:\n",
    "                logging.warning(\"[imgid=%s]download html failed.\"%self.imgid)\n",
    "                return False\n",
    "            \n",
    "            self.save_html(html, cache_file)\n",
    "            logging.info(\"[imgid=%s]download html successfully.\"%self.imgid)\n",
    "            \n",
    "        try:\n",
    "            self.soup = BeautifulSoup(html, 'html.parser')\n",
    "        except:\n",
    "            logging.error(\"[imgid=%s]Parse htmlSoup failed.\"%self.imgid)\n",
    "            return False\n",
    "        \n",
    "        return True\n",
    "\n",
    "    def save_html(self, html, cache_file, mod='w'):\n",
    "        with open(cache_file, mod) as fhtml:\n",
    "            fhtml.write(html)\n",
    "        \n",
    "    def save_image(self):\n",
    "        imgid = self.imgid\n",
    "        cached_img = self.img_cache_path + \"%s.jpg\"%imgid\n",
    "        if os.path.exists(cached_img):\n",
    "            logging.info(\"[imgid=%s]image has been cached.\"%imgid)\n",
    "        else:\n",
    "            img_url = self.get_img_url()\n",
    "            if img_url is None:\n",
    "                logging.warning(\"[imgid=%s]image does not exist.\"%imgid)\n",
    "                return False\n",
    "            \n",
    "            img = self.__get_html_response(img_url, valid_size=self.MIN_HTML_SIZE)\n",
    "            if img is None:\n",
    "                logging.warning(\"[imgid=%s]image caches failed.\"%imgid)\n",
    "                return False\n",
    "            \n",
    "            self.save_html(img, cached_img, mod='wb')\n",
    "            logging.info(\"[imgid=%s]image caches successfully.\"%imgid)\n",
    "        return True\n",
    "\n",
    "    def get_img_url(self):\n",
    "        img_container = self.soup.find(\"td\", id=\"img_container\")\n",
    "        if img_container is None or len(img_container.find_all(\"img\")) < 2:\n",
    "            return None\n",
    "        else:\n",
    "            return img_container.find_all(\"img\")[1].get(\"src\", None)\n",
    "        \n",
    "    def rm_failed_proxy(self):\n",
    "        for proxy, failed_cnt in self.failed_proxy.items():\n",
    "            if failed_cnt >= self.MAX_FAILED_CNT:\n",
    "                delete_proxy(proxy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "web_parser = WebParser(wait_second=0, max_retry_time=10)\n",
    "\n",
    "with open(\"./data/ava/AVA.txt\", 'r') as fin:\n",
    "    for line in tqdm(fin):\n",
    "        while len(get_all_proxy()) == 0:\n",
    "            logging.info(\"No avaliable proxy now\")\n",
    "            time.sleep(5*60)\n",
    "        \n",
    "        fields = line.strip().split(\" \")\n",
    "        imgid = fields[1]\n",
    "        \n",
    "        if web_parser.load_html(imgid):\n",
    "            web_parser.save_image()\n",
    "            \n",
    "        web_parser.rm_failed_proxy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "delete 0 images\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "img_path = \"./img_cache/\"\n",
    "count = 0\n",
    "for imgfile in os.listdir(img_path):\n",
    "    try:\n",
    "        img = Image.open(img_path+imgfile)\n",
    "    except:\n",
    "        os.remove(img_path+imgfile)\n",
    "        print \"delete %s\"%(img_path+imgfile)\n",
    "        count += 1\n",
    "        \n",
    "print \"delete %d images\"%count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
